{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import numpy as np\n",
    "import torchtext.vocab\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import data\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--tweet_sentiment_extraction-bee4b4571daa6a0e\n",
      "Reusing dataset json (C:\\Users\\20172613\\.cache\\huggingface\\datasets\\json\\SetFit--tweet_sentiment_extraction-bee4b4571daa6a0e\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28dc7bc099de4a9bbf2fdf8160f8b91b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--tweet_sentiment_extraction-bee4b4571daa6a0e\n",
      "Reusing dataset json (C:\\Users\\20172613\\.cache\\huggingface\\datasets\\json\\SetFit--tweet_sentiment_extraction-bee4b4571daa6a0e\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35a6c6f463174b1ab6a4010d8c6abb71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentiment_model.data_utils.tweet_dataset import TweetDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tweet_data = load_dataset(\"SetFit/tweet_sentiment_extraction\")\n",
    "tweet_dataset = TweetDataset(split='train')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sentiment_model.model import SentimentNet\n",
    "from sentiment_model.data_utils.tweet_dataset import pad_batch\n",
    "\n",
    "train_loader = DataLoader(tweet_dataset, batch_size=32, shuffle=True, collate_fn=pad_batch)\n",
    "len(tweet_dataset.vocab)\n",
    "\n",
    "model = SentimentNet()\n",
    "model_state_dict = torch.load(\"checkpoints/cool-bee-4/cool-bee-4-epoch-11.pth\")['model_state_dict']\n",
    "model.load_state_dict(model_state_dict)\n",
    "data, target, text_lengths = next(iter(train_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Token asfj;lsadj;lasdjf not found and default index is not set",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [54]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m tweet_dataset\u001B[38;5;241m.\u001B[39mvocab[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHello\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      3\u001B[0m tz \u001B[38;5;241m=\u001B[39m get_tokenizer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspacy\u001B[39m\u001B[38;5;124m\"\u001B[39m, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m \u001B[43mtweet_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43masfj;lsadj;lasdjf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\torchtext\\vocab\\vocab.py:64\u001B[0m, in \u001B[0;36mVocab.__getitem__\u001B[1;34m(self, token)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mexport\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, token: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;124;03m        token: The token used to lookup the corresponding index.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;124;03m        The index corresponding to the associated token.\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 64\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Token asfj;lsadj;lasdjf not found and default index is not set"
     ]
    }
   ],
   "source": [
    "tweet_dataset.vocab['Hello']\n",
    "\n",
    "tz = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "tweet_dataset.vocab['asfj;lsadj;lasdjf']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "['Helllooo', 'how', '.', 'lkjadf', ';', 'are', 'yuou\\\\', '?']"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz(\"Helllooo how. lkjadf; are yuou\\?\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "from torchtext. vocab import GloVe, vocab\n",
    "\n",
    "tokens = tz(\"Helllooo how. lkjadf; are yuou\\?\".lower())\n",
    "\n",
    "glove_twitter = GloVe(name = \"twitter.27B\", dim=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.4722,  0.6159, -0.6601, -0.1462, -0.4797, -0.5676,  0.4519,  0.4646,\n        -0.2835, -0.6584,  0.3741, -0.6437, -3.3311,  0.3133,  0.2019,  0.1715,\n         0.6436,  0.2090,  0.0325, -0.1899, -0.8975,  0.9616, -0.2340, -0.4361,\n         0.0687, -0.8583,  0.0737, -0.1705,  0.5671,  0.0952, -0.1938, -0.3952,\n         0.1049, -0.2117, -1.0626,  0.4975,  0.0692,  0.2275,  0.0324, -0.6410,\n        -0.5017,  0.0518,  0.3308,  0.7859,  1.3322, -0.0709,  0.6726,  0.2338,\n        -0.7912, -0.6818])"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_twitter.get_vecs_by_tokens()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "1193514"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vocab = vocab(glove_twitter.stoi)\n",
    "unk_token = \"<unk>\"\n",
    "unk_index = 0\n",
    "glove_vocab.insert_token(unk_token, unk_index)\n",
    "glove_vocab.set_default_index(unk_index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "if a is not None:\n",
    "    print(\"Yes\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "tz = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(tz.ids_to_tokens[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "sentiment",
   "language": "python",
   "display_name": "sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}